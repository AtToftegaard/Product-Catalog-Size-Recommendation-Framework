{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Dec 11 02:33:01 2017\n",
    "@author: rmisra\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pylmnn.lmnn import LargeMarginNearestNeighbor as LMNN\n",
    "from plots import plot_comparison\n",
    "from collections import defaultdict\n",
    "\n",
    "def parseData(file):\n",
    "    for l in open(file,'r'):\n",
    "        yield json.loads(l)\n",
    "        \n",
    "def remove_punctuation(text):\n",
    "    return ''.join([c.lower() for c in text if c not in set(string.punctuation)])\n",
    "\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of 1-LV"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Proposed in https://dl.acm.org/citation.cfm?id=3109891"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and split data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = list(parseData('datasets/renttherunway_final_data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fit': 'small',\n 'user_id': '185966',\n 'bust size': '34b',\n 'item_id': '1077123',\n 'weight': '135lbs',\n 'rating': '8',\n 'rented for': 'party',\n 'review_text': \"The dress arrived with a small hole in the beading on the front but wasn't too noticeable. Glad I was able to get two sizes because the 4 was a little tight and would've made for an uncomfortable night of dancing! \",\n 'body type': 'athletic',\n 'review_summary': \"It was fun to wear a dress I wouldn't normally buy! \",\n 'category': 'dress',\n 'height': '5\\' 3\"',\n 'size': 12,\n 'age': '33',\n 'review_date': 'January 2, 2018'}"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(154035, 19254, 19255)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "random.seed(1)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:int(0.8*len(data))]\n",
    "val_data = data[int(0.8*len(data)):int(0.9*len(data))]\n",
    "test_data = data[int(0.9*len(data)):]\n",
    "\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index user and items"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     6
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "item_data = {}\n",
    "item_index = {}\n",
    "user_index = {}\n",
    "user_data = {}\n",
    "u_index = 0\n",
    "i_index = 0\n",
    "for r in train_data:\n",
    "    if r['item_id'] + '|' + str(r['size']) not in item_data:\n",
    "        item_data[r['item_id'] + '|' + str(r['size'])] = [r]\n",
    "        item_index[r['item_id'] + '|' + str(r['size'])] = i_index\n",
    "        i_index += 1\n",
    "    else:\n",
    "        item_data[r['item_id'] + '|' + str(r['size'])].append(r)\n",
    "        \n",
    "    if r['user_id'] not in user_data:\n",
    "        user_data[r['user_id']] = [r]\n",
    "        user_index[r['user_id']] = u_index\n",
    "        u_index += 1\n",
    "    else:\n",
    "        user_data[r['user_id']].append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(90510, 90510, 28938, 28938)"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "len(user_data), len(user_index), len(item_data), len(item_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize items' true sizes with respective catalog sizes and users' true sizes randomly"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_size_item = np.zeros(len(item_data))\n",
    "true_size_cust = np.zeros(len(user_data))\n",
    "w = 1; b_1 = -1; b_2 = 1; lamda = 2\n",
    "\n",
    "for item in item_data:\n",
    "    true_size_item[item_index[item]] = int(item.split('|')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     22,
     57,
     60,
     72,
     84,
     98,
     101,
     130,
     162
    ],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def calc_auc():\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    for r in train_data:\n",
    "        fe = []\n",
    "        fe.append(true_size_cust[user_index[r['user_id']]])\n",
    "        fe.append(true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]])\n",
    "        train_features.append(fe)\n",
    "\n",
    "        if 'small' in r['fit']:\n",
    "            train_labels.append(0)\n",
    "        elif 'fit' in r['fit']:\n",
    "            train_labels.append(1)\n",
    "        elif 'large' in r['fit']:\n",
    "            train_labels.append(2)\n",
    "\n",
    "    c = 1\n",
    "    clf_1LV = LogisticRegression(fit_intercept=True, multi_class='ovr', C=c)\n",
    "    clf_1LV.fit(train_features, train_labels)\n",
    "\n",
    "    test_features = []; test_labels = []; test_labels_auc = []\n",
    "    for r in test_data:\n",
    "        fe = []\n",
    "        try:\n",
    "            fe.append(true_size_cust[user_index[r['user_id']]])\n",
    "        except KeyError:\n",
    "            fe.append(np.mean(true_size_cust))\n",
    "        try:\n",
    "            fe.append(true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]])\n",
    "        except KeyError:\n",
    "            fe.append(np.mean(true_size_item))\n",
    "\n",
    "        test_features.append(fe)\n",
    "        label = [0, 0, 0]\n",
    "        if 'small' in r['fit']:\n",
    "            test_labels.append(0)\n",
    "            label[0] = 1\n",
    "        elif 'fit' in r['fit']:\n",
    "            test_labels.append(1)\n",
    "            label[1] = 1\n",
    "        elif 'large' in r['fit']:\n",
    "            test_labels.append(2)\n",
    "            label[2] = 1\n",
    "        test_labels_auc.append(label)\n",
    "\n",
    "    test_labels_auc = np.array(test_labels_auc)\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "\n",
    "    pred = clf_1LV.predict_proba(test_features)\n",
    "    AUC = []\n",
    "    for i in range(3):\n",
    "        AUC.append(roc_auc_score(test_labels_auc[:,i], pred[:,i], average='weighted'))\n",
    "    print('Average AUC', np.mean(AUC), AUC)\n",
    "    \n",
    "def f(s,t):\n",
    "    return w*(s-t)\n",
    "\n",
    "def cal_loss_user(user, cust_size):\n",
    "    loss = 0\n",
    "    for r in user_data[user]:\n",
    "        if 'small' in r['fit']:\n",
    "            loss += max(0, 1 - f(cust_size, true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]]) + b_2)\n",
    "        elif 'fit' in r['fit']:\n",
    "            loss += max(0, 1 + f(cust_size, true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]]) - b_2)\n",
    "            loss += max(0, 1 - f(cust_size, true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]]) + b_1)\n",
    "        elif 'large' in r['fit']:\n",
    "            loss += max(0, 1 + f(cust_size, true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]]) - b_1)\n",
    "    return loss\n",
    "            \n",
    "def cal_loss_item(item, product_size):\n",
    "    loss = 0\n",
    "    for r in item_data[item]:\n",
    "        if 'small' in r['fit']:\n",
    "            loss += max(0, 1 - f(true_size_cust[user_index[r['user_id']]], product_size) + b_2)\n",
    "        elif 'fit' in r['fit']:\n",
    "            loss += max(0, 1 + f(true_size_cust[user_index[r['user_id']]], product_size) - b_2)\n",
    "            loss += max(0, 1 - f(true_size_cust[user_index[r['user_id']]], product_size) + b_1)\n",
    "        elif 'large' in r['fit']:\n",
    "            loss += max(0, 1 + f(true_size_cust[user_index[r['user_id']]], product_size) - b_1)\n",
    "    return loss\n",
    "\n",
    "def total_loss():\n",
    "    loss = 0\n",
    "    for item in item_data:\n",
    "        for r in item_data[item]:\n",
    "            product_size = true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]]\n",
    "            if 'small' in r['fit']:\n",
    "                loss += max(0, 1 - f(true_size_cust[user_index[r['user_id']]], product_size) + b_2)\n",
    "            elif 'fit' in r['fit']:\n",
    "                loss += max(0, 1 + f(true_size_cust[user_index[r['user_id']]], product_size) - b_2)\n",
    "                loss += max(0, 1 - f(true_size_cust[user_index[r['user_id']]], product_size) + b_1)\n",
    "            elif 'large' in r['fit']:\n",
    "                loss += max(0, 1 + f(true_size_cust[user_index[r['user_id']]], product_size) - b_1)\n",
    "    return loss\n",
    "\n",
    "for iterr in range(0,220):\n",
    "    \n",
    "    ## Phase 1\n",
    "    for user in user_data:\n",
    "        candidate_sizes = []\n",
    "        for r in user_data[user]:\n",
    "            if 'small' in r['fit']:\n",
    "                candidate_sizes.append(true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]] + ((b_2+1)/w))\n",
    "            elif 'fit' in r['fit']:\n",
    "                candidate_sizes.append(true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]] + ((b_1+1)/w))\n",
    "                candidate_sizes.append(true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]] + ((b_2-1)/w))\n",
    "            elif 'large' in r['fit']:\n",
    "                candidate_sizes.append(true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]] + ((b_1-1)/w))\n",
    "\n",
    "        flag = 0\n",
    "        candidate_sizes = list(set(candidate_sizes))\n",
    "        candidate_sizes = sorted(candidate_sizes)\n",
    "\n",
    "        if len(candidate_sizes) == 1:\n",
    "            true_size_cust[user_index[user]] = candidate_sizes[0]\n",
    "        else:\n",
    "            for s in range(1, len(candidate_sizes)):\n",
    "                slope = (cal_loss_user(user, candidate_sizes[s]) - cal_loss_user(user, candidate_sizes[s-1]))/(candidate_sizes[s] - candidate_sizes[s-1])\n",
    "                if slope>=0:\n",
    "                    flag=1\n",
    "                    true_size_cust[user_index[user]] = candidate_sizes[s-1]\n",
    "                    break\n",
    "\n",
    "            if flag==0:\n",
    "                true_size_cust[user_index[user]] = candidate_sizes[-1]\n",
    "\n",
    "    ## Phase 2\n",
    "    for item in item_data:\n",
    "        candidate_sizes = []\n",
    "        for r in item_data[item]:\n",
    "            if 'small' in r['fit']:\n",
    "                candidate_sizes.append(true_size_cust[user_index[r['user_id']]] - ((b_2+1)/w))\n",
    "            elif 'fit' in r['fit']:\n",
    "                candidate_sizes.append(true_size_cust[user_index[r['user_id']]] - ((b_1+1)/w))\n",
    "                candidate_sizes.append(true_size_cust[user_index[r['user_id']]] - ((b_2-1)/w))\n",
    "            elif 'large' in r['fit']:\n",
    "                candidate_sizes.append(true_size_cust[user_index[r['user_id']]] - ((b_1-1)/w))\n",
    "\n",
    "        flag = 0\n",
    "        candidate_sizes = list(set(candidate_sizes))\n",
    "        candidate_sizes = sorted(candidate_sizes)\n",
    "        if len(candidate_sizes) == 1:\n",
    "            true_size_item[item_index[item]] = candidate_sizes[0]\n",
    "        else:\n",
    "            for s in range(1, len(candidate_sizes)):\n",
    "                slope = (cal_loss_item(item, candidate_sizes[s]) - cal_loss_item(item, candidate_sizes[s-1]))/(candidate_sizes[s] - candidate_sizes[s-1])\n",
    "                if slope>=0:\n",
    "                    flag=1\n",
    "                    true_size_item[item_index[item]] = candidate_sizes[s-1]\n",
    "                    break\n",
    "\n",
    "            if flag==0:\n",
    "                true_size_item[item_index[item]] = candidate_sizes[-1]\n",
    "\n",
    "    ## Phase 3\n",
    "    learning_rate = 0.0000005/np.sqrt(iterr+1)\n",
    "    grad_w = 0\n",
    "    grad_b1 = 0\n",
    "    grad_b2 = 0\n",
    "    for r in train_data:\n",
    "        s = true_size_cust[user_index[r['user_id']]]\n",
    "        t = true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]]\n",
    "\n",
    "        if 'small' in r['fit']:\n",
    "            A = 1 - f(s, t) + b_2\n",
    "            if A>0:\n",
    "                grad_w += -1*(s - t)\n",
    "                grad_b2 += 1\n",
    "        elif 'fit' in r['fit']:\n",
    "            B = 1 + f(s, t) - b_2\n",
    "            C = 1 - f(s, t) + b_1\n",
    "            if B>0:\n",
    "                grad_w += (s - t)\n",
    "                grad_b2 += -1\n",
    "            if C>0:\n",
    "                grad_w += -1*(s - t)\n",
    "                grad_b1 += 1\n",
    "        elif 'large' in r['fit']:\n",
    "            D = 1 + f(s, t) - b_1\n",
    "            if D>0:\n",
    "                grad_w += (s - t)\n",
    "                grad_b1 += -1\n",
    "\n",
    "    w -= learning_rate*(grad_w + 2*lamda*w)\n",
    "    b_1 -= learning_rate*(grad_b1 + 2*lamda*b_1)\n",
    "    b_2 -= learning_rate*(grad_b2 + 2*lamda*b_2)\n",
    "    if iterr%5 == 0:\n",
    "        print(iterr, total_loss())\n",
    "        calc_auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the above learned features in a classifier"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     23,
     37,
     40,
     52
    ],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "train_features = []; train_labels = []\n",
    "for r in train_data:\n",
    "    fe = []\n",
    "    fe.append(true_size_cust[user_index[r['user_id']]])\n",
    "    fe.append(true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]])\n",
    "    train_features.append(fe)\n",
    "\n",
    "    if 'small' in r['fit']:\n",
    "        train_labels.append(0)\n",
    "    elif 'fit' in r['fit']:\n",
    "        train_labels.append(1)\n",
    "    elif 'large' in r['fit']:\n",
    "        train_labels.append(2)\n",
    "\n",
    "c = 1\n",
    "clf_1LV = LogisticRegression(fit_intercept=True, multi_class='ovr', C=c)\n",
    "clf_1LV.fit(train_features, train_labels)\n",
    "\n",
    "test_features = []; test_labels = []; test_labels_auc = []\n",
    "for r in val_data:\n",
    "    fe = []\n",
    "    try:\n",
    "        u = user_index[r['user_id']]\n",
    "        fe.append(true_size_cust[u])\n",
    "    except KeyError:\n",
    "        fe.append(np.mean(true_size_cust))\n",
    "    try:\n",
    "        fe.append(true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]])\n",
    "    except KeyError:\n",
    "        fe.append(np.mean(true_size_item))\n",
    "\n",
    "    test_features.append(fe)\n",
    "    label = [0, 0, 0]\n",
    "    if 'small' in r['fit']:\n",
    "        test_labels.append(0)\n",
    "        label[0] = 1\n",
    "    elif 'fit' in r['fit']:\n",
    "        test_labels.append(1)\n",
    "        label[1] = 1\n",
    "    elif 'large' in r['fit']:\n",
    "        test_labels.append(2)\n",
    "        label[2] = 1\n",
    "    test_labels_auc.append(label)\n",
    "\n",
    "test_labels_auc = np.array(test_labels_auc)\n",
    "\n",
    "pred = clf_1LV.predict_proba(test_features)\n",
    "AUC = []\n",
    "for i in range(3):\n",
    "    AUC.append(roc_auc_score(test_labels_auc[:,i], pred[:,i], average='weighted'))\n",
    "print('Average AUC', np.mean(AUC), AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of K-LF-ML"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create users' and items' (parent and child) indices"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     11
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "item_data = {}; user_data = {}\n",
    "item_index = {}; user_index = {}; p_item_index = {} ## p_ is for parent \n",
    "u_index = 0; i_index = 0; p_i_index = 0\n",
    "sizes = {}\n",
    "s_index = 0\n",
    "\n",
    "for r in data:\n",
    "    if r['size'] not in sizes:\n",
    "        sizes[r['size']] = s_index\n",
    "        s_index += 1\n",
    "\n",
    "    if r['item_id'] + '|' + str(r['size']) not in item_data:\n",
    "        item_data[r['item_id'] + '|' + str(r['size'])] = [r]\n",
    "        item_index[r['item_id'] + '|' + str(r['size'])] = i_index\n",
    "        i_index += 1\n",
    "    else:\n",
    "        item_data[r['item_id'] + '|' + str(r['size'])].append(r)\n",
    "        \n",
    "    if r['item_id'] not in p_item_index:\n",
    "        p_item_index[r['item_id']] = p_i_index\n",
    "        p_i_index += 1\n",
    "        \n",
    "    if r['user_id'] not in user_data:\n",
    "        user_data[r['user_id']] = [r]\n",
    "        user_index[r['user_id']] = u_index\n",
    "        u_index += 1\n",
    "    else:\n",
    "        user_data[r['user_id']].append(r)\n",
    "len(user_data), len(user_index), len(item_data), len(item_index), len(p_item_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For monotonicity constraints, for each product, record smaller and larger sized products, if any"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     8,
     15
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_sizes = {}; product_sizes_rev = {}\n",
    "\n",
    "for r in data:\n",
    "    if r['item_id'] not in product_sizes:\n",
    "        product_sizes[r['item_id']] = [r['size']]\n",
    "    else:\n",
    "        product_sizes[r['item_id']].append(r['size'])\n",
    "        \n",
    "for k in product_sizes:\n",
    "    product_sizes[k] = list(sorted(set(product_sizes[k])))\n",
    "    product_sizes_rev[k] = list(sorted(set(product_sizes[k]), reverse=True))\n",
    "    \n",
    "product_smaller = {}\n",
    "product_larger = {}\n",
    "\n",
    "for k in product_sizes:\n",
    "    for i in range(len(product_sizes[k])):\n",
    "        if len(product_sizes[k]) == 1:\n",
    "            product_smaller[item_index[k + '|' + str(product_sizes[k][i])]] = -1\n",
    "            product_larger[item_index[k + '|' + str(product_sizes[k][i])]] = -1\n",
    "        elif i == 0:\n",
    "            product_smaller[item_index[k + '|' + str(product_sizes[k][i])]] = -1\n",
    "            product_larger[item_index[k + '|' + str(product_sizes[k][i])]] = item_index[k + '|' + str(product_sizes[k][i+1])]\n",
    "        elif i == len(product_sizes[k]) - 1:\n",
    "            product_smaller[item_index[k + '|' + str(product_sizes[k][i])]] = item_index[k + '|' + str(product_sizes[k][i-1])]\n",
    "            product_larger[item_index[k + '|' + str(product_sizes[k][i])]] = -1\n",
    "        else:\n",
    "            product_smaller[item_index[k + '|' + str(product_sizes[k][i])]] = item_index[k + '|' + str(product_sizes[k][i-1])]\n",
    "            product_larger[item_index[k + '|' + str(product_sizes[k][i])]] = item_index[k + '|' + str(product_sizes[k][i+1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompose fit semantics using latent factor model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(a,b_i,b_u,s,t):\n",
    "    return np.dot(w, np.concatenate(([a, b_u, b_i], np.multiply(s, t))))\n",
    "\n",
    "K = 10; learning_rate = 0.000005; alpha = 1; \n",
    "true_size_item = np.random.normal(size = (len(item_index), K))*0.1\n",
    "true_size_cust = np.random.normal(size = (len(user_index), K), loc=1, scale=0.1)\n",
    "bias_i = np.random.normal(size = (len(item_index)))*0.1\n",
    "bias_u = np.random.normal(size = (len(user_index)))*0.1\n",
    "b_1 = -5; b_2 = 5; lamda = 2; w = np.ones(K+3)\n",
    "\n",
    "for k in product_sizes:\n",
    "    start = len(product_sizes[k])\n",
    "    for size in product_sizes[k]:\n",
    "        true_size_item[item_index[k + '|' + str(size)]] = np.random.normal(size=(1,K), loc=start, scale=0.1)\n",
    "        start -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_auc():\n",
    "    train_features = []; train_labels = []\n",
    "    for r in train_data:\n",
    "        fe = []\n",
    "        fe.append(w[1]*bias_u[user_index[r['user_id']]])\n",
    "        fe.append(w[2]*bias_i[p_item_index[r['item_id']]])\n",
    "        fe.extend(np.multiply(w[3:], np.multiply(true_size_cust[user_index[r['user_id']]], true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]])))\n",
    "\n",
    "        train_features.append(fe)\n",
    "\n",
    "        if 'small' in r['fit']:\n",
    "            train_labels.append(1)\n",
    "        elif 'fit' in r['fit']:\n",
    "            train_labels.append(2)\n",
    "        elif 'large' in r['fit']:\n",
    "            train_labels.append(3)\n",
    "\n",
    "    clf = LogisticRegression(fit_intercept=True, multi_class='ovr')\n",
    "    clf.fit(train_features, train_labels)\n",
    "\n",
    "    test_features = []; test_labels = []; test_labels_auc = []\n",
    "    for r in test_data:\n",
    "        fe = []\n",
    "        flag = 0\n",
    "        fe.append(w[1]*bias_u[user_index[r['user_id']]])\n",
    "        fe.append(w[2]*bias_i[p_item_index[r['item_id']]])\n",
    "        fe.extend(np.multiply(w[3:], np.multiply(true_size_cust[user_index[r['user_id']]], true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]])))\n",
    "\n",
    "        test_features.append(fe)\n",
    "        label = [0, 0, 0]\n",
    "        if 'small' in r['fit']:\n",
    "            test_labels.append(1)\n",
    "            label[0] = 1\n",
    "        elif 'fit' in r['fit']:\n",
    "            test_labels.append(2)\n",
    "            label[1] = 1\n",
    "        elif 'large' in r['fit']:\n",
    "            test_labels.append(3)\n",
    "            label[2] = 1\n",
    "        test_labels_auc.append(label)\n",
    "\n",
    "    test_labels_auc = np.array(test_labels_auc)\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    pred = clf.predict_proba(test_features)\n",
    "    AUC = []\n",
    "    for i in range(3):\n",
    "        AUC.append(roc_auc_score(test_labels_auc[:,i], pred[:,i], average='weighted'))\n",
    "    print('Average AUC', np.mean(AUC), AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_metric_auc():\n",
    "    U = true_size_cust; V = true_size_item\n",
    "    def prepare_features(data):\n",
    "        X = []\n",
    "        Y = []\n",
    "        Y_auc = []\n",
    "        item_l = []\n",
    "        item_n = []\n",
    "        items = {}\n",
    "        item_count = defaultdict(int)\n",
    "        item_small = defaultdict(int)\n",
    "        frac_small = []\n",
    "        for r in data:\n",
    "            fe = []\n",
    "            fe.append(w[1]*bias_u[user_index[r['user_id']]])\n",
    "            fe.append(w[2]*bias_i[p_item_index[r['item_id']]])\n",
    "            fe.extend(np.multiply(w[3:], np.multiply(true_size_cust[user_index[r['user_id']]], true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]])))\n",
    "            X.append(fe)\n",
    "            item_l.append(np.multiply(w[3:],V[item_index[r['item_id'] + '|' + str(r['size'])]]))\n",
    "            item_n.append(str(r['category']))\n",
    "            items[r['item_id'] + '|' + str(r['size'])] = 1\n",
    "            if 'small' in r['fit']:\n",
    "                Y.append(0)\n",
    "                Y_auc.append([1,0,0])\n",
    "            elif 'fit' in r['fit']:\n",
    "                Y.append(1)\n",
    "                Y_auc.append([0,1,0])\n",
    "            else:\n",
    "                Y.append(2)\n",
    "                Y_auc.append([0,0,1])\n",
    "\n",
    "        for r in train_data:\n",
    "            if (r['item_id'] + '|' + str(r['size'])) in items:\n",
    "                item_count[r['item_id'] + '|' + str(r['size'])] += 1\n",
    "                if 'small' in r['fit']:\n",
    "                    item_small[r['item_id'] + '|' + str(r['size'])] += 1\n",
    "        for k in item_count:\n",
    "            item_small[k] = item_small[k]/item_count[k]\n",
    "\n",
    "        for r in data:\n",
    "            frac_small.append(item_small[r['item_id'] + '|' + str(r['size'])])\n",
    "\n",
    "        return np.array(X),np.array(Y), np.array(Y_auc), np.array(item_l), item_n, frac_small\n",
    "    \n",
    "    def select_prototype(data):\n",
    "        chosen_data = []\n",
    "        X_small,_,_,_,_,_ = prepare_features(data)\n",
    "        small_mean = np.mean(X_small, axis = 0)\n",
    "        dist = []\n",
    "        for i in range(len(data)):\n",
    "            dist.append((i, sum([(X_small[i][j]-small_mean[j])**2 for j in range(len(small_mean))])))\n",
    "        if 'fit' in data[0]['fit']:\n",
    "            small_temp = sorted(dist, key=operator.itemgetter(1))[1300:]\n",
    "            interval = int(len(data)/5000)\n",
    "            for i in range(100):\n",
    "                chosen_data.append(data[small_temp[int(i*interval)][0]])\n",
    "                interval += 4\n",
    "        elif 'small' in data[0]['fit']:\n",
    "            small_temp = sorted(dist, key=operator.itemgetter(1))[1200:]\n",
    "            interval = int(len(data)/1000)\n",
    "            for i in range(100):\n",
    "                chosen_data.append(data[small_temp[int(i*interval)][0]])\n",
    "                interval += 1.15\n",
    "        else:\n",
    "            small_temp = sorted(dist, key=operator.itemgetter(1))[1200:]\n",
    "            interval = int(len(data)/500)\n",
    "            for i in range(100):\n",
    "                chosen_data.append(data[small_temp[int(i*interval)][0]])\n",
    "                interval += 0.5\n",
    "\n",
    "        return chosen_data\n",
    "\n",
    "    small = []; true = []; large= []\n",
    "    for r in range(len(train_data)):\n",
    "        if 'small' in train_data[r]['fit']:\n",
    "            small.append(train_data[r])\n",
    "        elif 'fit' in train_data[r]['fit']:\n",
    "            true.append(train_data[r])\n",
    "        else:\n",
    "            large.append(train_data[r])\n",
    "\n",
    "    random.shuffle(small); random.shuffle(true); random.shuffle(large)\n",
    "    data_training = [];\n",
    "    data_training.extend(select_prototype(small)); data_training.extend(select_prototype(true)); data_training.extend(select_prototype(large))\n",
    "\n",
    "    random.shuffle(data_training)\n",
    "    X_train, Y_train, Y_train_auc, item_l, item_n, frac_small = prepare_features(data_training)\n",
    "    X_test, Y_test, Y_test_auc, _, _, _ = prepare_features(test_data)\n",
    "\n",
    "    clf_kLF = LMNN(n_neighbors=53, max_iter=50, n_features_out=X_train.shape[1], verbose=0)\n",
    "    clf_kLF = clf_kLF.fit(X_train, Y_train)\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    pred = clf_kLF.predict_proba(X_test); AUC = []\n",
    "    for i in range(3):\n",
    "        AUC.append(roc_auc_score(Y_test_auc[:,i], pred[:,i], average='weighted'))\n",
    "    print('Average AUC', np.mean(AUC), AUC)\n",
    "    return np.mean(AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected Gradient Descent"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     18
    ],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def total_loss():\n",
    "    loss = 0\n",
    "    for item in item_data:\n",
    "        for r in item_data[item]:\n",
    "            s = true_size_cust[user_index[r['user_id']]]\n",
    "            t = true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]]\n",
    "            b_i = bias_i[p_item_index[r['item_id']]]\n",
    "            b_u = bias_u[user_index[r['user_id']]]\n",
    "            \n",
    "            if 'small' in r['fit']:\n",
    "                loss += max(0, 1 - f(alpha,b_i,b_u,s,t) + b_2)\n",
    "            elif 'fit' in r['fit']:\n",
    "                loss += max(0, 1 + f(alpha,b_i,b_u,s,t) - b_2)\n",
    "                loss += max(0, 1 - f(alpha,b_i,b_u,s,t) + b_1)\n",
    "            elif 'large' in r['fit']:\n",
    "                loss += max(0, 1 + f(alpha,b_i,b_u,s,t) - b_1)\n",
    "    return loss\n",
    "\n",
    "for iterr in range(0,450):\n",
    "    learning_rate1 = 0.00025\n",
    "    learning_rate2 = 0.00001/np.sqrt(iterr+1)\n",
    "    \n",
    "    grad_b1 = 0\n",
    "    grad_b2 = 0\n",
    "    grad_s = np.zeros((len(user_index), K))\n",
    "    grad_t = np.zeros((len(item_index), K))\n",
    "    grad_bu = np.zeros(len(user_index))\n",
    "    grad_bi = np.zeros(len(p_item_index))\n",
    "    grad_bs = np.zeros(len(sizes))\n",
    "    grad_w = np.zeros((K+3))\n",
    "    for r in train_data:\n",
    "        s = true_size_cust[user_index[r['user_id']]]\n",
    "        t = true_size_item[item_index[r['item_id'] + '|' + str(r['size'])]]\n",
    "        b_i = bias_i[p_item_index[r['item_id']]]\n",
    "        b_u = bias_u[user_index[r['user_id']]]\n",
    "        \n",
    "        if 'small' in r['fit']:\n",
    "            A = 1 - f(alpha,b_i,b_u,s,t) + b_2\n",
    "            if A>0:\n",
    "                grad_s[user_index[r['user_id']]] += -1*np.multiply(w[3:],t)\n",
    "                grad_t[item_index[r['item_id'] + '|' + str(r['size'])]] += -1*np.multiply(w[3:],s)\n",
    "                \n",
    "                grad_bu[user_index[r['user_id']]] += -1*w[1]\n",
    "                grad_bi[p_item_index[r['item_id']]] += -1*w[2]\n",
    "                grad_w += -1*np.concatenate(([alpha, b_u, b_i], np.multiply(s, t)))\n",
    "                grad_b2 += 1\n",
    "        elif 'fit' in r['fit']:\n",
    "            B = 1 + f(alpha,b_i,b_u,s,t) - b_2\n",
    "            C = 1 - f(alpha,b_i,b_u,s,t) + b_1\n",
    "            if B>0:\n",
    "                grad_s[user_index[r['user_id']]] += np.multiply(w[3:],t)\n",
    "                grad_t[item_index[r['item_id'] + '|' + str(r['size'])]] += np.multiply(w[3:],s)\n",
    "                \n",
    "                grad_bu[user_index[r['user_id']]] += w[1]\n",
    "                grad_bi[p_item_index[r['item_id']]] += w[2]\n",
    "                grad_w += np.concatenate(([alpha, b_u, b_i], np.multiply(s, t)))\n",
    "                grad_b2 += -1\n",
    "            if C>0:\n",
    "                grad_s[user_index[r['user_id']]] += -1*np.multiply(w[3:],t)\n",
    "                grad_t[item_index[r['item_id'] + '|' + str(r['size'])]] += -1*np.multiply(w[3:],s)\n",
    "                \n",
    "                grad_bu[user_index[r['user_id']]] += -1*w[1]\n",
    "                grad_bi[p_item_index[r['item_id']]] += -1*w[2]\n",
    "                grad_w += -1*np.concatenate(([alpha, b_u, b_i], np.multiply(s, t)))\n",
    "                grad_b1 += 1\n",
    "        elif 'large' in r['fit']:\n",
    "            D = 1 + f(alpha,b_i,b_u,s,t) - b_1\n",
    "            if D>0:\n",
    "                grad_s[user_index[r['user_id']]] += np.multiply(w[3:],t)\n",
    "                grad_t[item_index[r['item_id'] + '|' + str(r['size'])]] += np.multiply(w[3:],s)\n",
    "                \n",
    "                grad_bu[user_index[r['user_id']]] += w[1]\n",
    "                grad_bi[p_item_index[r['item_id']]] += w[2]\n",
    "                grad_w += np.concatenate(([alpha, b_u, b_i], np.multiply(s, t)))\n",
    "                grad_b1 += -1\n",
    "\n",
    "    for i in range(len(user_index)):\n",
    "        true_size_cust[i] -= learning_rate1*(grad_s[i] + 2*lamda*true_size_cust[i])\n",
    "        bias_u[i] -= learning_rate1*(grad_bu[i] + 2*lamda*bias_u[i])\n",
    "        \n",
    "    for i in range(len(item_index)):\n",
    "        ## constraint update\n",
    "        temp = true_size_item[i] - learning_rate1*(grad_t[i] + 2*lamda*true_size_item[i])\n",
    "        if product_smaller[i] != -1:\n",
    "            temp = np.maximum(temp, true_size_item[product_smaller[i]])\n",
    "        if product_larger[i] != -1:\n",
    "            temp = np.minimum(temp, true_size_item[product_larger[i]])\n",
    "        true_size_item[i] = temp\n",
    "        \n",
    "    for i in range(len(p_item_index)):\n",
    "        bias_i[i] -= learning_rate1*(grad_bi[i] + 2*lamda*bias_i[i])        \n",
    "    \n",
    "    b_1 -= learning_rate2*(grad_b1 + 2*lamda*b_1)\n",
    "    b_2 -= learning_rate2*(grad_b2 + 2*lamda*b_2)\n",
    "    w -= learning_rate2*(grad_w + 2*lamda*w)\n",
    "    if iterr%5 == 0:    \n",
    "        print(iterr, total_loss(), b_1, b_2,w[:4])\n",
    "        calc_auc()\n",
    "        auc = calc_metric_auc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calc_metric_auc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('venv': venv)",
   "language": "python",
   "name": "python38164bitvenvvenv1e6a60fc1cd44f3b8d943fec987c6a08"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}